{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makhmudov-khondamir/Machine-Learning-Projects/blob/main/Sentimental_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O08MmJsvTKaV"
      },
      "outputs": [],
      "source": [
        "#dont forget to connect to GPU, otherwise doesn't work\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BtWMwbs6Tf9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd815f53-a5b1-4bce-ecfd-3f9d627d3580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"hf://datasets/adkhamboy/sentiment-uz/sentiment_uz.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "aEsQW4ebTf_h",
        "outputId": "4a8d3583-0bb7-4d56-cefd-1b23bf4af7b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  label\n",
              "0                                  uni yaxshi ko'raman\\n      1\n",
              "1      bunga qo'shimcha qilish kerak bo'lgan yuklab o...      1\n",
              "2                                  bepul bepul qo'shiq\\n      1\n",
              "3      kyla saka ushbu o'yinlarni o'ynashni yaxshi ko...      1\n",
              "4      juda ham ajoyib. bugungi kunga qadar men 36 ta...      1\n",
              "...                                                  ...    ...\n",
              "18480           karaoke ishlamayapti. o'zgarish kerak.\\n      0\n",
              "18481  juda tez donduruyor juda tez-tez muzlashadi. o...      0\n",
              "18482  menga o'ynashga yo'l qo'ymaydi menga mehmon si...      0\n",
              "18483  garbage app buzildi. yaxshi ishlash uchun foyd...      0\n",
              "18484  nima bo'ladi ushodlikni qanday bildirilmoqni q...      0\n",
              "\n",
              "[18485 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5733c812-fbdf-4df6-b28f-84bfe8b7e080\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>uni yaxshi ko'raman\\n</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bunga qo'shimcha qilish kerak bo'lgan yuklab o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bepul bepul qo'shiq\\n</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kyla saka ushbu o'yinlarni o'ynashni yaxshi ko...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>juda ham ajoyib. bugungi kunga qadar men 36 ta...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18480</th>\n",
              "      <td>karaoke ishlamayapti. o'zgarish kerak.\\n</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18481</th>\n",
              "      <td>juda tez donduruyor juda tez-tez muzlashadi. o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18482</th>\n",
              "      <td>menga o'ynashga yo'l qo'ymaydi menga mehmon si...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18483</th>\n",
              "      <td>garbage app buzildi. yaxshi ishlash uchun foyd...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18484</th>\n",
              "      <td>nima bo'ladi ushodlikni qanday bildirilmoqni q...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18485 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5733c812-fbdf-4df6-b28f-84bfe8b7e080')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5733c812-fbdf-4df6-b28f-84bfe8b7e080 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5733c812-fbdf-4df6-b28f-84bfe8b7e080');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ba42b375-5694-4708-8d20-7d05d5f7c8e5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ba42b375-5694-4708-8d20-7d05d5f7c8e5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ba42b375-5694-4708-8d20-7d05d5f7c8e5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_22f33ca5-ebde-4954-8d84-62e0acfe75c3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_22f33ca5-ebde-4954-8d84-62e0acfe75c3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 18485,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17913,\n        \"samples\": [\n          \"yaxshi to'laydi.\\n\",\n          \"bu sof shit agar men berish imkoniyati 0 bo'lishi mumkin bo'lsa, men unga berilishi mumkin edi .va hamma odamlar plz yuklab olish emas, balki ....\\n\",\n          \"men hech qachon bir tiyinga tegmagan eng oddiy darajadagi dizaynlardan biri va hatto bepul bo'lsa ham shubhali\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns=['sentence','label']"
      ],
      "metadata": {
        "id": "8LgyqHDUjaAa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LHiTk79XTgHs"
      },
      "outputs": [],
      "source": [
        "data = data.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iFch759JTgCM"
      },
      "outputs": [],
      "source": [
        "data['sentence'] = data['sentence'].apply(lambda x: x.lower())\n",
        "data['sentence'] = data['sentence'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "#This line cleans the text in the sentence column by removing any non-alphanumeric characters (e.g., punctuation, emojis) and keeping only letters, numbers, and spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL_8VqglTgE9",
        "outputId": "e4856375-4a7d-4e1d-9108-b8b3e8558299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17706\n",
            "19264\n"
          ]
        }
      ],
      "source": [
        "print(data[ data['label'] == 1].size)\n",
        "print(data[ data['label'] == 0].size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['label'].value_counts()"
      ],
      "metadata": {
        "id": "5vZPvlhyAfL4",
        "outputId": "7bb44b05-ce3e-4045-c522-400c945c0589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    9632\n",
              "1    8853\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8853</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, here is a balance between two classes (almost 52% and 48%). if it was kinda 82% and 18%, big difference, the model wouldn't work well, resulting overfitting or underfitting problems"
      ],
      "metadata": {
        "id": "uHPOYRalAkqO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zPOAQ5P6UMvA",
        "outputId": "01e78ede-fcd3-48df-9a10-de01db050cda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  label\n",
              "0  savol va javob yomon xonaga qanday qilib chodi...      1\n",
              "1  togri emas faqat notogri manzil emas balki not...      0\n",
              "2  qulaymani tolayman men ilovaga tolayman nima u...      0\n",
              "3                      bosh idish yaxshi toza oyin\\n      1\n",
              "4  soliq lekin multiplayer muvaffaqiyati boladi m...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e09e4cda-e1e6-4eda-b7a6-1e5fd2b27d46\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>savol va javob yomon xonaga qanday qilib chodi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>togri emas faqat notogri manzil emas balki not...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>qulaymani tolayman men ilovaga tolayman nima u...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bosh idish yaxshi toza oyin\\n</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>soliq lekin multiplayer muvaffaqiyati boladi m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e09e4cda-e1e6-4eda-b7a6-1e5fd2b27d46')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e09e4cda-e1e6-4eda-b7a6-1e5fd2b27d46 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e09e4cda-e1e6-4eda-b7a6-1e5fd2b27d46');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6c84ed70-a65c-40a3-a953-ad98f3096376\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c84ed70-a65c-40a3-a953-ad98f3096376')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6c84ed70-a65c-40a3-a953-ad98f3096376 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 18485,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17882,\n        \"samples\": [\n          \"albatta uning ildizlarini korsatib turibdi  bu eski flash oyining portidir va albatta bu grafikada korsatilgan bundan tashqari juda tez bir vaqtning ozida juda monotonik boladi\\n\",\n          \"yoshim kichikroq bolganimda bu oyinni hech qachon tugata olmadim lekin hozirda ajoyib oyinim bolishi mumkin ammo men usbbt planshet klaviaturalari uchun klaviatura nazorati mavjudligini xohlayman\\n\",\n          \"glich men har doim kurashga tushgan oyinni oynabman u mening eng yaxshi oyinga 5 va mening yonimdagi oyinchi mening hisobimni oladi it toldirilishi uchun 91 va ulugdigan va muvofiq oyinchi muqaddas boladimi\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MDDZbo3lUNYr"
      },
      "outputs": [],
      "source": [
        "max_fatures = 2000                                         # ONLY TAKE THE MOST REPEATED WORDS\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')    # formula\n",
        "tokenizer.fit_on_texts(data['sentence'].values)            # apply that formula on our data column, result will be like mapping: {'laptop': 1, 'I': 2, 'love': 3, ...}\n",
        "X = tokenizer.texts_to_sequences(data['sentence'].values)  # rewrite all sentences in all rows by using specific tokenized 2000 words, words which are not included in those 2000 ares imply dropped and not tokenized\n",
        "X = pad_sequences(X)                                       # this is used to ensure that all sequences (lists of tokenized integers) have the same length by adding 0 (if )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation**\n",
        "\n",
        "### 1) Why `max_features = 2000`? Why not less or more? What's the purpose?\n",
        "\n",
        "- **Purpose**: `max_features` specifies the maximum number of unique words (features) to consider from the dataset. If you set `max_features = 2000`, the tokenizer will consider only the top 2000 most frequent words in your text data and ignore the rest.\n",
        "  \n",
        "- **Why 2000?**:\n",
        "  - **Trade-off**: This number is chosen based on the size of the dataset, computational resources, and the modelâ€™s complexity.\n",
        "  - **Less than 2000**: If you choose a smaller value (e.g., 500), the tokenizer will ignore many potentially useful words, leading to loss of important information and reducing the model's ability to understand the data.\n",
        "  - **More than 2000**: If you choose a larger value (e.g., 10000), the tokenizer will include less frequent words, increasing the dimensionality and complexity of the model. This can lead to overfitting, especially if you have a small dataset.\n",
        "  \n",
        "  **Example**:\n",
        "  - For a **small dataset**, setting `max_features = 500` might be enough because there wonâ€™t be many unique words.\n",
        "  - For a **large dataset** like a collection of news articles or social media posts, setting `max_features = 2000` or more would help capture enough vocabulary diversity without overloading the model.\n",
        "\n",
        "  The choice of `2000` is often a balance between capturing enough information and managing model complexity. Itâ€™s a hyperparameter that you can tune based on your specific dataset and task.\n",
        "\n",
        "### 2) In the tokenizer, are integers for each unique word given at that moment or pre-built in tokenizer vocabulary for specific words?\n",
        "\n",
        "- **Answer**: The integers are **generated at that moment** based on the vocabulary of your specific dataset. The tokenizer creates a vocabulary by analyzing your text data after calling `fit_on_texts()`. It assigns a unique integer to each word based on its frequency in the dataset.\n",
        "  \n",
        "  **How it works**:\n",
        "  - The tokenizer starts with an empty vocabulary.\n",
        "  - When you call `fit_on_texts(data['text'].values)`, the tokenizer scans your text data and builds a vocabulary where each word is mapped to a unique integer.\n",
        "  - The most frequent word is assigned the integer `1`, the second most frequent word is assigned `2`, and so on, until it reaches the limit defined by `max_features`.\n",
        "\n",
        "  **Example**:\n",
        "  - Suppose your text data contains the following sentences: `[\"I love coding\", \"I love AI\"]`.\n",
        "  - After fitting, the tokenizer might assign integers like:\n",
        "    - `\"I\"` â†’ 1\n",
        "    - `\"love\"` â†’ 2\n",
        "    - `\"coding\"` â†’ 3\n",
        "    - `\"AI\"` â†’ 4\n",
        "\n",
        "### 3) `X = pad_sequences(X)` - The purpose of this is to convert a dense vector into sparse by filling empty spaces with `0` to ensure equal length for minimum-length words, right?\n",
        "\n",
        "- **Not exactly**. The purpose of `pad_sequences()` is to ensure **equal length** for all sequences by adding padding (usually zeros) to sequences that are **shorter** than the maximum length. It is not about converting a dense vector into a sparse one but about standardizing the input data for the neural network.\n",
        "\n",
        "- **Why padding**:\n",
        "  - Neural networks require input sequences of the same length, but text sequences in real-world data can vary in length.\n",
        "  - **`pad_sequences()`** ensures that all sequences have the same length by either **padding** shorter sequences with zeros (at the beginning or end) or **truncating** longer sequences to match the desired length.\n",
        "  \n",
        "  **Example**:\n",
        "  - Suppose you have two sequences:\n",
        "    - Sequence 1: `[1, 2, 3]`\n",
        "    - Sequence 2: `[4, 5]`\n",
        "  - After padding, they might look like:\n",
        "    - Sequence 1: `[1, 2, 3]` (no padding needed)\n",
        "    - Sequence 2: `[0, 4, 5]` (padded with one `0` at the beginning)\n",
        "\n",
        "  This padding ensures that the network can process both sequences as inputs of the same length.\n",
        "\n",
        "-------------------------------------------------------\n",
        "### X = tokenizer.texts_to_sequences(data['sentence'].values)\n",
        "### 1) **Understanding Tokenization**:\n",
        "\n",
        "Yes, tokenization involves assigning unique integers to words, but **it's based on frequency of occurrence**:\n",
        "\n",
        "- **Most frequent word**: The word that appears the most (e.g., \"laptop\" if it appears 8000 times) gets the lowest integer, such as `1`.\n",
        "- **Less frequent words**: Words that appear less frequently are given progressively higher integers. For instance, the 2000th most frequent word (e.g., \"hi\", appearing 200 times) might get assigned the integer `2000`.\n",
        "\n",
        "When you set `max_features = 2000`, it means the tokenizer will **only consider the top 2000 most frequent words**. Any word beyond that, like your example of \"bye\" (which might be ranked 2001st based on frequency), will not be included in the model.\n",
        "\n",
        "### 2) **Handling Rare but Important Words**:\n",
        "\n",
        "The challenge here is that rare words may carry significant meaning, especially in reviews where specific sentiments or details might hinge on less frequent words. For example, in a hotel review, words like \"quaint\", \"underwhelming\", or \"damp\" might be rare but crucial to the sentiment.\n",
        "\n",
        "Here are a few strategies to handle this issue:\n",
        "\n",
        "#### **A) Adjusting `max_features`**:\n",
        "- **Increase `max_features`**: You can try increasing the `max_features` parameter to include more rare words. For example, if you suspect that many rare words are important in your dataset, you might want to try `4000`, `6000`, or even `8000` as the value.\n",
        "  \n",
        "  **Drawback**: Increasing `max_features` will increase the dimensionality of your model, leading to more complex and computationally expensive training. You also risk overfitting to rare words that may not generalize well.\n",
        "\n",
        "#### **B) Using Word Embeddings**:\n",
        "- **Word embeddings** (like **Word2Vec** or **GloVe**) can help capture semantic meaning, including for rare words. These embeddings map words into dense vectors that represent relationships between words based on context, rather than purely on frequency.\n",
        "  \n",
        "  **Benefit**: Word embeddings allow the model to generalize better, as even rare words will be embedded in a space where they have a meaningful relationship with other words. For example, \"quaint\" might be close to \"charming\" in the embedding space, so even if \"quaint\" is rare, the model can still capture its sentiment based on its relation to similar words.\n",
        "\n",
        "#### **C) Subword Tokenization**:\n",
        "- **Subword tokenization** (like **Byte Pair Encoding (BPE)** or **WordPiece**) is used in models like BERT. Instead of treating each word as a unique token, it breaks words down into smaller subword units. This way, even rare words can be partially represented by more common subwords.\n",
        "\n",
        "  **Example**: The word \"underwhelming\" might be broken down into \"under\", \"whelm\", and \"ing\", allowing the model to recognize it even if it hasn't seen the full word before.\n",
        "\n",
        "#### **D) Data Augmentation**:\n",
        "- You can augment your dataset by adding more examples with rare but important words. This can help balance the distribution and ensure that rare words aren't overlooked due to their low frequency.\n",
        "\n",
        "### **Conclusion**:\n",
        "\n",
        "- **Adjusting `max_features`** might be a simple first step, but if rare words are truly crucial to your modelâ€™s performance, you should consider using **word embeddings** or **subword tokenization** to help the model better understand the relationships between words, regardless of their frequency.\n",
        "--------------------------------------------------\n",
        "The tokenizer focuses on the top `2000` most frequent words when you set `max_features=2000`. Now let's dive into what happens to words that fall outside this range (beyond the 2000 most frequent words).\n",
        "\n",
        "### **What Happens to Words Beyond the Top 2000?**\n",
        "\n",
        "1. **Ignored Words**:\n",
        "   - Any word that is not in the top `2000` most frequent words is **ignored** during the `texts_to_sequences` conversion. These words are effectively removed from the sequences.\n",
        "   - If a sentence contains words that are outside the top `2000` frequent words, those words wonâ€™t be converted into integers. They will simply be **excluded from the sequence**.\n",
        "\n",
        "2. **Example**:\n",
        "   Let's say you have a sentence that includes rare words that are outside the top `2000` words. For example:\n",
        "\n",
        "   ```python\n",
        "   sentence = \"I love my ultrabook\"\n",
        "   ```\n",
        "\n",
        "   Assume that:\n",
        "   - \"I\", \"love\", and \"my\" are within the top `2000` most frequent words, so they will be tokenized.\n",
        "   - \"ultrabook\" is a rare word and falls outside the top `2000`, so it will **not** be tokenized.\n",
        "\n",
        "   The conversion result will look like this:\n",
        "\n",
        "   ```python\n",
        "   [2, 3, 4]  # \"I\" â†’ 2, \"love\" â†’ 3, \"my\" â†’ 4, \"ultrabook\" is ignored\n",
        "   ```\n",
        "\n",
        "   The word \"ultrabook\" is excluded from the sequence because it is beyond the top `2000` frequent words.\n",
        "\n",
        "### **Pros and Cons of Ignoring Rare Words**:\n",
        "\n",
        "#### **Pros**:\n",
        "1. **Simplicity**: Limiting the vocabulary size (e.g., to 2000 words) reduces the complexity of the model and the size of the input sequences, making the training process faster.\n",
        "2. **Noise Reduction**: Rare words often don't contribute much to the overall model and may act as noise in some cases, especially in large datasets.\n",
        "\n",
        "#### **Cons**:\n",
        "1. **Loss of Important Information**: Rare but important words might carry critical information (especially in nuanced text, like reviews). For example, words like \"dysfunctional\" or \"exceptional\" might be rare but highly indicative of sentiment.\n",
        "\n",
        "### **How to Handle This Situation?**\n",
        "\n",
        "There are a few strategies you can use to deal with rare words:\n",
        "\n",
        "1. **Increase `max_features`**:\n",
        "   - You can increase the `max_features` value from `2000` to something larger (e.g., `4000`, `5000`, or more). This will allow more words to be included in your vocabulary, reducing the number of ignored words.\n",
        "\n",
        "2. **Use Embeddings**:\n",
        "   - Word embeddings like Word2Vec, GloVe, or BERT can help in dealing with rare words because they are pre-trained on larger corpora. They provide vector representations for words that are not frequent in your dataset but might still appear in the pre-trained vocabulary.\n",
        "\n",
        "3. **Out-of-Vocabulary (OOV) Token**:\n",
        "   - You can configure the tokenizer to use an **OOV token** for words that are outside the top `2000` words. This ensures that instead of dropping the rare words, they are replaced by a special token like `<OOV>`. Here's how you can do it:\n",
        "\n",
        "     ```python\n",
        "     tokenizer = Tokenizer(num_words=max_fatures, oov_token=\"<OOV>\", split=' ')\n",
        "     ```\n",
        "\n",
        "     This way, any word outside the top `2000` will be replaced by `<OOV>` and mapped to a specific integer, preventing the complete loss of those words in your sequences.\n",
        "\n",
        "### **Conclusion**:\n",
        "Words outside the top `2000` most frequent ones are excluded from the sequences unless you take steps to increase `max_features` or use an OOV token. Balancing the trade-off between keeping rare words and reducing model complexity is key to designing an effective solution.\n",
        "\n"
      ],
      "metadata": {
        "id": "0CbyYo2-yMhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "word_index        # to see the full mapping, print it"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWSRnBZJmHdi",
        "outputId": "00a8d99b-6d40-4b2f-eb44-f02fc989fdcd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'va': 1,\n",
              " 'men': 2,\n",
              " 'yaxshi': 3,\n",
              " 'bu': 4,\n",
              " 'uchun': 5,\n",
              " 'juda': 6,\n",
              " 'oyin': 7,\n",
              " 'uni': 8,\n",
              " 'lekin': 9,\n",
              " 'u': 10,\n",
              " 'oyinni': 11,\n",
              " 'ajoyib': 12,\n",
              " 'bir': 13,\n",
              " 'ushbu': 14,\n",
              " 'hech': 15,\n",
              " 'kerak': 16,\n",
              " 'bilan': 17,\n",
              " 'ham': 18,\n",
              " 'emas': 19,\n",
              " 'koraman': 20,\n",
              " 'har': 21,\n",
              " 'mening': 22,\n",
              " 'eng': 23,\n",
              " 'mumkin': 24,\n",
              " 'qanday': 25,\n",
              " 'yoq': 26,\n",
              " 'siz': 27,\n",
              " 'faqat': 28,\n",
              " 'kop': 29,\n",
              " 'katta': 30,\n",
              " 'narsa': 31,\n",
              " 'yoki': 32,\n",
              " 'dastur': 33,\n",
              " 'qiziqarli': 34,\n",
              " 'buni': 35,\n",
              " 'iltimos': 36,\n",
              " 'sotib': 37,\n",
              " 'yomon': 38,\n",
              " 'edi': 39,\n",
              " 'keyin': 40,\n",
              " 'agar': 41,\n",
              " 'yuklab': 42,\n",
              " 'menga': 43,\n",
              " 'endi': 44,\n",
              " 'chunki': 45,\n",
              " 'nima': 46,\n",
              " 'ammo': 47,\n",
              " 'boshqa': 48,\n",
              " 'ilovani': 49,\n",
              " 'ilova': 50,\n",
              " 'barcha': 51,\n",
              " 'kabi': 52,\n",
              " 'ishlamaydi': 53,\n",
              " 'bor': 54,\n",
              " '5': 55,\n",
              " 'olish': 56,\n",
              " 'pul': 57,\n",
              " 'yangi': 58,\n",
              " 'shuning': 59,\n",
              " 'ega': 60,\n",
              " 'qachon': 61,\n",
              " 'bolgan': 62,\n",
              " 'hatto': 63,\n",
              " 'koproq': 64,\n",
              " 'bolsa': 65,\n",
              " 'deb': 66,\n",
              " 'marta': 67,\n",
              " 'sizning': 68,\n",
              " 'vaqt': 69,\n",
              " 'qilish': 70,\n",
              " 'uning': 71,\n",
              " 'qayta': 72,\n",
              " 'kulgili': 73,\n",
              " 'song': 74,\n",
              " 'harakat': 75,\n",
              " 'oynash': 76,\n",
              " 'ular': 77,\n",
              " 'albatta': 78,\n",
              " 'hali': 79,\n",
              " 'ishlaydi': 80,\n",
              " 'bolishi': 81,\n",
              " 'doim': 82,\n",
              " 'oynashni': 83,\n",
              " 'qaytarib': 84,\n",
              " 'bepul': 85,\n",
              " '2': 86,\n",
              " 'davom': 87,\n",
              " 'yordam': 88,\n",
              " 'qilaman': 89,\n",
              " 'qilib': 90,\n",
              " 'boladi': 91,\n",
              " 'dahshatli': 92,\n",
              " 'ochirib': 93,\n",
              " 'yana': 94,\n",
              " 'olib': 95,\n",
              " 'app': 96,\n",
              " 'zor': 97,\n",
              " 'qiyin': 98,\n",
              " 'sizni': 99,\n",
              " 'oson': 100,\n",
              " '3': 101,\n",
              " 'necha': 102,\n",
              " 'togri': 103,\n",
              " 'ruxsat': 104,\n",
              " 'pulni': 105,\n",
              " 'bolalar': 106,\n",
              " 'meni': 107,\n",
              " 'narsalarni': 108,\n",
              " 'olmayman': 109,\n",
              " 'rahmat': 110,\n",
              " 'hamma': 111,\n",
              " 'faqatgina': 112,\n",
              " 'qiladi': 113,\n",
              " 'dasturni': 114,\n",
              " 'beradi': 115,\n",
              " 'oldim': 116,\n",
              " 'narsani': 117,\n",
              " 'yulduz': 118,\n",
              " 'zerikarli': 119,\n",
              " 'reklama': 120,\n",
              " 'birinchi': 121,\n",
              " 'ishlamayapti': 122,\n",
              " 'oz': 123,\n",
              " 'boldi': 124,\n",
              " 'tashqari': 125,\n",
              " 'bundan': 126,\n",
              " 'bazi': 127,\n",
              " 'reklamalar': 128,\n",
              " 'haqida': 129,\n",
              " 'galaxy': 130,\n",
              " 'ta': 131,\n",
              " 'haqiqiy': 132,\n",
              " 'oldin': 133,\n",
              " 'oynashga': 134,\n",
              " 'sevgi': 135,\n",
              " 'olishni': 136,\n",
              " 'oyini': 137,\n",
              " 'ilovalar': 138,\n",
              " 'sizga': 139,\n",
              " 'olmaydi': 140,\n",
              " 'mavjud': 141,\n",
              " 'xohlayman': 142,\n",
              " 'toliq': 143,\n",
              " 'biz': 144,\n",
              " 'ularni': 145,\n",
              " '1': 146,\n",
              " 'yoqdi': 147,\n",
              " 'yaxshiroq': 148,\n",
              " 'notogri': 149,\n",
              " 'ish': 150,\n",
              " 'ishlab': 151,\n",
              " 'javob': 152,\n",
              " 'muammo': 153,\n",
              " 'qadar': 154,\n",
              " 'narsalar': 155,\n",
              " 'qila': 156,\n",
              " 'korish': 157,\n",
              " 'vaqtni': 158,\n",
              " '4': 159,\n",
              " 'android': 160,\n",
              " 'shu': 161,\n",
              " 'safar': 162,\n",
              " 'olaman': 163,\n",
              " 'erda': 164,\n",
              " 'qoshimcha': 165,\n",
              " 'oyinlar': 166,\n",
              " 'ishga': 167,\n",
              " 'davomida': 168,\n",
              " 'sevaman': 169,\n",
              " 'bermaydi': 170,\n",
              " 'hozir': 171,\n",
              " 'balki': 172,\n",
              " 'yoqimli': 173,\n",
              " 'ajablanarlisi': 174,\n",
              " 'tavsiya': 175,\n",
              " 'qiling': 176,\n",
              " 'xil': 177,\n",
              " 'qildim': 178,\n",
              " 'oyinlarni': 179,\n",
              " 'oddiy': 180,\n",
              " 'samsung': 181,\n",
              " 'sinab': 182,\n",
              " 'bolib': 183,\n",
              " 'oyinda': 184,\n",
              " 'bitta': 185,\n",
              " 'oyinlari': 186,\n",
              " 'tuzatish': 187,\n",
              " 'yangilanish': 188,\n",
              " 'beraman': 189,\n",
              " 'boldim': 190,\n",
              " 'oling': 191,\n",
              " 'kora': 192,\n",
              " 'bolmaydi': 193,\n",
              " 'songgi': 194,\n",
              " 'umid': 195,\n",
              " 'yol': 196,\n",
              " 'xato': 197,\n",
              " 'chiroyli': 198,\n",
              " 'ikki': 199,\n",
              " 'sevimli': 200,\n",
              " 'uzoq': 201,\n",
              " 'grafikalar': 202,\n",
              " 'ilovasi': 203,\n",
              " 'sevadi': 204,\n",
              " 'talab': 205,\n",
              " 'buning': 206,\n",
              " 'bering': 207,\n",
              " 'edim': 208,\n",
              " 'hal': 209,\n",
              " 'shunday': 210,\n",
              " 'oynab': 211,\n",
              " 'beri': 212,\n",
              " 'biri': 213,\n",
              " 'oxirgi': 214,\n",
              " 'korinadi': 215,\n",
              " 'ishni': 216,\n",
              " 'xafa': 217,\n",
              " 'pulimni': 218,\n",
              " 'oylayman': 219,\n",
              " 'amalga': 220,\n",
              " 'tez': 221,\n",
              " 'tuzatib': 222,\n",
              " 'qanchadanqancha': 223,\n",
              " 'nafratlanaman': 224,\n",
              " 'yangilash': 225,\n",
              " 'olmaysiz': 226,\n",
              " 'kichik': 227,\n",
              " 'qora': 228,\n",
              " 'qizim': 229,\n",
              " 'oynagan': 230,\n",
              " 'qoying': 231,\n",
              " 'qabul': 232,\n",
              " 'biroq': 233,\n",
              " 'yulduzga': 234,\n",
              " 'nechta': 235,\n",
              " 'google': 236,\n",
              " 'oynaydi': 237,\n",
              " 'olmadim': 238,\n",
              " 'mukammal': 239,\n",
              " 'yoshli': 240,\n",
              " 'orqali': 241,\n",
              " 'emasman': 242,\n",
              " 'odamlar': 243,\n",
              " 'tomosha': 244,\n",
              " 'qilmaydi': 245,\n",
              " 'foydali': 246,\n",
              " 'ekran': 247,\n",
              " 'yuqori': 248,\n",
              " 'sekin': 249,\n",
              " 'foydalanish': 250,\n",
              " 'olishingiz': 251,\n",
              " 'ravishda': 252,\n",
              " 'deyarli': 253,\n",
              " 'tushadi': 254,\n",
              " 'bezovta': 255,\n",
              " 'bosh': 256,\n",
              " 'menda': 257,\n",
              " '10': 258,\n",
              " 'qilishni': 259,\n",
              " 'eski': 260,\n",
              " 'tuzating': 261,\n",
              " 'qaytib': 262,\n",
              " 'plz': 263,\n",
              " 'doimo': 264,\n",
              " 'kordim': 265,\n",
              " 'yulduzni': 266,\n",
              " 'ketadi': 267,\n",
              " 'turadi': 268,\n",
              " 'galaba': 269,\n",
              " 'aniq': 270,\n",
              " 'bunday': 271,\n",
              " 'esa': 272,\n",
              " 'turli': 273,\n",
              " 'mobil': 274,\n",
              " 'nexus': 275,\n",
              " 'biroz': 276,\n",
              " 'xarid': 277,\n",
              " 'ularning': 278,\n",
              " 'qorqinchli': 279,\n",
              " 'holda': 280,\n",
              " 'ochilmaydi': 281,\n",
              " 'koplab': 282,\n",
              " 'versiyani': 283,\n",
              " 'istayman': 284,\n",
              " 'bilmayman': 285,\n",
              " 'undan': 286,\n",
              " 'kun': 287,\n",
              " 'oxshash': 288,\n",
              " 'muammolar': 289,\n",
              " 'haqiqatan': 290,\n",
              " 'butunlay': 291,\n",
              " 'ovoz': 292,\n",
              " 'hozirda': 293,\n",
              " 'ishlatish': 294,\n",
              " 'usuli': 295,\n",
              " 'kerakligini': 296,\n",
              " 'ha': 297,\n",
              " 'ehtimol': 298,\n",
              " 'bolsangiz': 299,\n",
              " 'korib': 300,\n",
              " 'ichida': 301,\n",
              " 'umuman': 302,\n",
              " 'oynay': 303,\n",
              " 'da': 304,\n",
              " 'ok': 305,\n",
              " 'butun': 306,\n",
              " 'aslida': 307,\n",
              " 'oyindan': 308,\n",
              " 'oyinkulgi': 309,\n",
              " 'toxtatib': 310,\n",
              " 'yangilanishdan': 311,\n",
              " 'kim': 312,\n",
              " 'ni': 313,\n",
              " 'ijro': 314,\n",
              " 'koradi': 315,\n",
              " 'bilaman': 316,\n",
              " 'soat': 317,\n",
              " 'qoladi': 318,\n",
              " 'telefon': 319,\n",
              " 'biror': 320,\n",
              " 'shuningdek': 321,\n",
              " 'oglim': 322,\n",
              " 'qoydim': 323,\n",
              " 'keladi': 324,\n",
              " 'bunga': 325,\n",
              " 'qiladigan': 326,\n",
              " 'ilovalarni': 327,\n",
              " 'dan': 328,\n",
              " 'keyingi': 329,\n",
              " 'telefonim': 330,\n",
              " 'qilsam': 331,\n",
              " 'darajada': 332,\n",
              " 'dasturi': 333,\n",
              " 'super': 334,\n",
              " '6': 335,\n",
              " 'tashlash': 336,\n",
              " 'qoshing': 337,\n",
              " 'bolmadi': 338,\n",
              " 'asosiy': 339,\n",
              " 'his': 340,\n",
              " 'mos': 341,\n",
              " 'qilamanki': 342,\n",
              " 'foydasiz': 343,\n",
              " 'nazorat': 344,\n",
              " 'oladi': 345,\n",
              " 'muhtoj': 346,\n",
              " 'musiqa': 347,\n",
              " 'oynashim': 348,\n",
              " 'xuddi': 349,\n",
              " 'ga': 350,\n",
              " 'koring': 351,\n",
              " 'kuni': 352,\n",
              " 'ornatish': 353,\n",
              " 'aytadi': 354,\n",
              " 'yopiladi': 355,\n",
              " 'ustida': 356,\n",
              " 'ilovaga': 357,\n",
              " 'yil': 358,\n",
              " 'ochirish': 359,\n",
              " 'telefonimni': 360,\n",
              " 'obhavo': 361,\n",
              " 'oyinini': 362,\n",
              " 'ilovada': 363,\n",
              " 'arziydi': 364,\n",
              " 'oyinning': 365,\n",
              " 'afsuski': 366,\n",
              " 'kirish': 367,\n",
              " 'nihoyat': 368,\n",
              " 'ushlab': 369,\n",
              " 'telefonimda': 370,\n",
              " 'versiya': 371,\n",
              " 'yoqtiraman': 372,\n",
              " 'oyinlardan': 373,\n",
              " 'taklif': 374,\n",
              " 'yoqotib': 375,\n",
              " 'olishim': 376,\n",
              " 'tolayman': 377,\n",
              " 'qulab': 378,\n",
              " 'it': 379,\n",
              " 'berish': 380,\n",
              " 'sababli': 381,\n",
              " 'majbur': 382,\n",
              " 'sifatida': 383,\n",
              " 'wifi': 384,\n",
              " 'olgan': 385,\n",
              " 'soxta': 386,\n",
              " 'reklamalarni': 387,\n",
              " 'bolardi': 388,\n",
              " 'bazan': 389,\n",
              " 'qulay': 390,\n",
              " 'muammoni': 391,\n",
              " 'baxtli': 392,\n",
              " 'yagona': 393,\n",
              " 'yoqadi': 394,\n",
              " 'avtomatik': 395,\n",
              " 'hozirgi': 396,\n",
              " 'sodir': 397,\n",
              " 'unga': 398,\n",
              " 'ochish': 399,\n",
              " 'darhol': 400,\n",
              " 'yangilanishlar': 401,\n",
              " 'olmayapman': 402,\n",
              " 'versiyasi': 403,\n",
              " 'elektron': 404,\n",
              " 'sevib': 405,\n",
              " 'ortiq': 406,\n",
              " 'berishni': 407,\n",
              " 'malumot': 408,\n",
              " 'pro': 409,\n",
              " 'bolsam': 410,\n",
              " 'ochib': 411,\n",
              " 'yoqotish': 412,\n",
              " 'kutib': 413,\n",
              " 'versiyasini': 414,\n",
              " 'korishni': 415,\n",
              " 'bugungi': 416,\n",
              " 'teztez': 417,\n",
              " 'tushib': 418,\n",
              " 'pulga': 419,\n",
              " 'topa': 420,\n",
              " 'sucks': 421,\n",
              " 'duch': 422,\n",
              " 'pastga': 423,\n",
              " 'qodir': 424,\n",
              " 'oylaymanki': 425,\n",
              " 'urinib': 426,\n",
              " '30': 427,\n",
              " 'qoldim': 428,\n",
              " 'oyinim': 429,\n",
              " 'tomonidan': 430,\n",
              " 'ekranga': 431,\n",
              " 'shunga': 432,\n",
              " 'yanada': 433,\n",
              " 'kam': 434,\n",
              " 'muammolarni': 435,\n",
              " 'qoyadi': 436,\n",
              " 'qilishim': 437,\n",
              " 'bolar': 438,\n",
              " 'qilmang': 439,\n",
              " 'hozirgacha': 440,\n",
              " 'berdi': 441,\n",
              " '7': 442,\n",
              " 'xursand': 443,\n",
              " 'video': 444,\n",
              " 'boshqalar': 445,\n",
              " 'allaqachon': 446,\n",
              " 'yoqtirmayman': 447,\n",
              " 'tufayli': 448,\n",
              " 'ur': 449,\n",
              " 'pochta': 450,\n",
              " 'qoshadi': 451,\n",
              " 'oxshaydi': 452,\n",
              " 'dollar': 453,\n",
              " 'tezda': 454,\n",
              " 'jiddiy': 455,\n",
              " 'darajalar': 456,\n",
              " 'oyinchi': 457,\n",
              " 'ekranni': 458,\n",
              " 'vau': 459,\n",
              " 'ilovadan': 460,\n",
              " 'saqlab': 461,\n",
              " 'bolish': 462,\n",
              " 'ayniqsa': 463,\n",
              " 'bolmagan': 464,\n",
              " 'telefonimga': 465,\n",
              " 'qarshi': 466,\n",
              " 'korgan': 467,\n",
              " 'aks': 468,\n",
              " 'pulli': 469,\n",
              " 'xabar': 470,\n",
              " 'pls': 471,\n",
              " 'qoshimchalar': 472,\n",
              " 'xatolar': 473,\n",
              " 'onlayn': 474,\n",
              " 'qildi': 475,\n",
              " 'turing': 476,\n",
              " 'tezkor': 477,\n",
              " 'joy': 478,\n",
              " 'xatolik': 479,\n",
              " 'paydo': 480,\n",
              " 'chindan': 481,\n",
              " 'imkon': 482,\n",
              " 'keyinchalik': 483,\n",
              " 'etadi': 484,\n",
              " 'tarzda': 485,\n",
              " 'ahmoq': 486,\n",
              " 'bolmasa': 487,\n",
              " 'nafrat': 488,\n",
              " 'besh': 489,\n",
              " 'kunda': 490,\n",
              " 'cool': 491,\n",
              " 'kimdir': 492,\n",
              " 'oyinlarini': 493,\n",
              " 'uy': 494,\n",
              " 'soz': 495,\n",
              " 'bilib': 496,\n",
              " 'shunchaki': 497,\n",
              " 'behuda': 498,\n",
              " 'yuklamaydi': 499,\n",
              " 'etarli': 500,\n",
              " 'foydalanuvchi': 501,\n",
              " 'hammasi': 502,\n",
              " 'kirishga': 503,\n",
              " '100': 504,\n",
              " 'qanchalik': 505,\n",
              " 'baholayman': 506,\n",
              " 'lg': 507,\n",
              " 'grafika': 508,\n",
              " 'topish': 509,\n",
              " 'boshladim': 510,\n",
              " 'organish': 511,\n",
              " 'axlat': 512,\n",
              " 'doimiy': 513,\n",
              " 'bekor': 514,\n",
              " 'daqiqada': 515,\n",
              " 'htc': 516,\n",
              " 'tasodifiy': 517,\n",
              " 'tolash': 518,\n",
              " 'yangilashdan': 519,\n",
              " 'qilmoqda': 520,\n",
              " 'bolsada': 521,\n",
              " 'qilmayman': 522,\n",
              " 'reyting': 523,\n",
              " 'oynayman': 524,\n",
              " 'i': 525,\n",
              " 'ishlash': 526,\n",
              " 'pulingizni': 527,\n",
              " 'yangilab': 528,\n",
              " 'eting': 529,\n",
              " 'yaqin': 530,\n",
              " 'daqiqa': 531,\n",
              " 'buyon': 532,\n",
              " 'kishi': 533,\n",
              " 'oynashingiz': 534,\n",
              " 'fix': 535,\n",
              " 'korganman': 536,\n",
              " 'xizmat': 537,\n",
              " 'bolalarim': 538,\n",
              " 'xohlagan': 539,\n",
              " 'otish': 540,\n",
              " 'qolgan': 541,\n",
              " 'wtf': 542,\n",
              " 'yangilanishni': 543,\n",
              " 'olmang': 544,\n",
              " 'd': 545,\n",
              " 'qoshib': 546,\n",
              " 'awsome': 547,\n",
              " 'ishlaydigan': 548,\n",
              " 'olasiz': 549,\n",
              " 'chiqadi': 550,\n",
              " 'barchasi': 551,\n",
              " 'sizda': 552,\n",
              " 'hafta': 553,\n",
              " 'kamroq': 554,\n",
              " 'haqda': 555,\n",
              " 'qongiroq': 556,\n",
              " 'qoyish': 557,\n",
              " 'qilsangiz': 558,\n",
              " 'daqiqadan': 559,\n",
              " 'judayam': 560,\n",
              " 'oshirish': 561,\n",
              " 'jumboq': 562,\n",
              " 'kutish': 563,\n",
              " 'yuklash': 564,\n",
              " 'amazing': 565,\n",
              " 'tashladim': 566,\n",
              " 'ketdi': 567,\n",
              " 'ozingizning': 568,\n",
              " 'oy': 569,\n",
              " 'ikkinchi': 570,\n",
              " 'bogliq': 571,\n",
              " 'narsaga': 572,\n",
              " 'boshlash': 573,\n",
              " 'qisqa': 574,\n",
              " 'menimcha': 575,\n",
              " 'lol': 576,\n",
              " 'vaqtning': 577,\n",
              " 'birga': 578,\n",
              " 'ochiladi': 579,\n",
              " 'muzlatib': 580,\n",
              " 'dostlarim': 581,\n",
              " 'yaxshisi': 582,\n",
              " 'telefonni': 583,\n",
              " '20': 584,\n",
              " 'korsatadi': 585,\n",
              " 'ozimning': 586,\n",
              " 'qaytarishni': 587,\n",
              " 'love': 588,\n",
              " 'poyga': 589,\n",
              " 'qoshiq': 590,\n",
              " 'berishi': 591,\n",
              " 'olishga': 592,\n",
              " 'foydalaning': 593,\n",
              " 'eslatma': 594,\n",
              " 'qaerda': 595,\n",
              " 'omg': 596,\n",
              " 'ikkita': 597,\n",
              " 'qilishingiz': 598,\n",
              " 'belgilar': 599,\n",
              " 'internetga': 600,\n",
              " 'qaytaring': 601,\n",
              " 'orniga': 602,\n",
              " 'kelmaydi': 603,\n",
              " 'uch': 604,\n",
              " 'taxminan': 605,\n",
              " 'bolganda': 606,\n",
              " 'oqish': 607,\n",
              " 'vaqtdan': 608,\n",
              " 'odamlarni': 609,\n",
              " 'qulaydi': 610,\n",
              " '0': 611,\n",
              " 'ajablanarli': 612,\n",
              " 'mumkinligini': 613,\n",
              " 'kichkina': 614,\n",
              " '8': 615,\n",
              " 'qilyapman': 616,\n",
              " 'topdim': 617,\n",
              " 'bazida': 618,\n",
              " 'paytda': 619,\n",
              " '15': 620,\n",
              " 'daraja': 621,\n",
              " 'ketgan': 622,\n",
              " 'aqlli': 623,\n",
              " 'n': 624,\n",
              " 'soniya': 625,\n",
              " 'darajaga': 626,\n",
              " 'pianino': 627,\n",
              " 'toxtata': 628,\n",
              " 'sarflamang': 629,\n",
              " 'loyiq': 630,\n",
              " 'bajarish': 631,\n",
              " 'qoydi': 632,\n",
              " 'oldi': 633,\n",
              " 'download': 634,\n",
              " 'oyinga': 635,\n",
              " 'muhim': 636,\n",
              " 'bolaman': 637,\n",
              " 'marotaba': 638,\n",
              " 'shirin': 639,\n",
              " 'qattiq': 640,\n",
              " 'bolishini': 641,\n",
              " 'ochishga': 642,\n",
              " 'qilingan': 643,\n",
              " 'nega': 644,\n",
              " 'ekanligini': 645,\n",
              " 'vaqtini': 646,\n",
              " 'bizning': 647,\n",
              " 'etish': 648,\n",
              " 'qaramay': 649,\n",
              " 'yuk': 650,\n",
              " 'istagan': 651,\n",
              " 'note': 652,\n",
              " 'tashqariga': 653,\n",
              " 'qolga': 654,\n",
              " 'tashakkur': 655,\n",
              " 'etilgan': 656,\n",
              " 'oltin': 657,\n",
              " 'tushirish': 658,\n",
              " 'kecha': 659,\n",
              " 'keldi': 660,\n",
              " 'ishlamadi': 661,\n",
              " 'oq': 662,\n",
              " 'tezroq': 663,\n",
              " 'beradigan': 664,\n",
              " 'aqldan': 665,\n",
              " 'tab': 666,\n",
              " 'oyladim': 667,\n",
              " 'odam': 668,\n",
              " 'ularga': 669,\n",
              " 'oynadim': 670,\n",
              " 'maxsus': 671,\n",
              " 'qaytadan': 672,\n",
              " 'yarim': 673,\n",
              " 'beshta': 674,\n",
              " 'qismini': 675,\n",
              " 'ishlayotgan': 676,\n",
              " 'boshqarish': 677,\n",
              " 'qoldi': 678,\n",
              " 'maslahat': 679,\n",
              " 'so': 680,\n",
              " 'bolishni': 681,\n",
              " 'qoyaman': 682,\n",
              " 'taqdim': 683,\n",
              " 'tuzatishga': 684,\n",
              " 'tolov': 685,\n",
              " 'xursandman': 686,\n",
              " 'qoshiqni': 687,\n",
              " 'nimani': 688,\n",
              " 's6': 689,\n",
              " 'ahmoqona': 690,\n",
              " 'dont': 691,\n",
              " 'aytdi': 692,\n",
              " 's5': 693,\n",
              " 'shikoyat': 694,\n",
              " 'boshlanadi': 695,\n",
              " 'tushdi': 696,\n",
              " 'bartaraf': 697,\n",
              " 'stupid': 698,\n",
              " 'ochirilgan': 699,\n",
              " 'shundaki': 700,\n",
              " 'bola': 701,\n",
              " 'tugmasi': 702,\n",
              " 's4': 703,\n",
              " 'qoygan': 704,\n",
              " 'bizni': 705,\n",
              " 'tiklash': 706,\n",
              " 'korsatish': 707,\n",
              " 'vaqtida': 708,\n",
              " 'internet': 709,\n",
              " 'tashlayman': 710,\n",
              " 'olsam': 711,\n",
              " 'joyga': 712,\n",
              " 'yomoni': 713,\n",
              " 'kimga': 714,\n",
              " 'turish': 715,\n",
              " 'etiladi': 716,\n",
              " 'ishlamayapman': 717,\n",
              " 'qollabquvvatlash': 718,\n",
              " 'ishonchli': 719,\n",
              " 'tolashingiz': 720,\n",
              " 'fikr': 721,\n",
              " 'sarflash': 722,\n",
              " 'yulduzli': 723,\n",
              " 'oziga': 724,\n",
              " 'qilgan': 725,\n",
              " 'yillar': 726,\n",
              " 'etarlicha': 727,\n",
              " 'tolovni': 728,\n",
              " 'odatda': 729,\n",
              " 'dam': 730,\n",
              " 'telefonda': 731,\n",
              " 'tolangan': 732,\n",
              " 'oynata': 733,\n",
              " 'nafaqat': 734,\n",
              " 'jonli': 735,\n",
              " 'malumotlarni': 736,\n",
              " 'bermayman': 737,\n",
              " 'etmoqda': 738,\n",
              " 'ozimni': 739,\n",
              " 'a': 740,\n",
              " 'ulanish': 741,\n",
              " 'sarf': 742,\n",
              " 'aloqa': 743,\n",
              " 'shaxmat': 744,\n",
              " 'otgan': 745,\n",
              " 'boyicha': 746,\n",
              " 'ettiradi': 747,\n",
              " 'muammosi': 748,\n",
              " 'qoymaydi': 749,\n",
              " 'kopincha': 750,\n",
              " 'play': 751,\n",
              " 'berilmaydi': 752,\n",
              " 'kunga': 753,\n",
              " 'tizimga': 754,\n",
              " 'chiqish': 755,\n",
              " 'ishlay': 756,\n",
              " '50': 757,\n",
              " 'dastlabki': 758,\n",
              " 'hozircha': 759,\n",
              " 'oyinlarga': 760,\n",
              " 'yozib': 761,\n",
              " 'foydalanilgan': 762,\n",
              " 'telefonimdagi': 763,\n",
              " 'kira': 764,\n",
              " 'muammolari': 765,\n",
              " 'muvaffaqiyatsiz': 766,\n",
              " 'qiziq': 767,\n",
              " 'crap': 768,\n",
              " 'tekshirib': 769,\n",
              " 'ochganimda': 770,\n",
              " 'bermadi': 771,\n",
              " 'yuboradi': 772,\n",
              " 'uchta': 773,\n",
              " 'yuklaydi': 774,\n",
              " 'avval': 775,\n",
              " 'facebook': 776,\n",
              " 'tuzatadi': 777,\n",
              " 'qoshish': 778,\n",
              " 'qaram': 779,\n",
              " 'toxtaydi': 780,\n",
              " 'royxatdan': 781,\n",
              " 'yuz': 782,\n",
              " 'kamida': 783,\n",
              " 'tanlash': 784,\n",
              " 'kuchli': 785,\n",
              " 'qilolmaysiz': 786,\n",
              " 'yildan': 787,\n",
              " 'bonus': 788,\n",
              " 'ishladi': 789,\n",
              " 'past': 790,\n",
              " 'cheklangan': 791,\n",
              " 'abadiy': 792,\n",
              " 'hayajonli': 793,\n",
              " 'oyinlarning': 794,\n",
              " 'yoshda': 795,\n",
              " 'sharmandalik': 796,\n",
              " 'qulflangan': 797,\n",
              " 'ekranda': 798,\n",
              " 'oynaganimda': 799,\n",
              " 'vaqti': 800,\n",
              " 'muvaffaqiyatli': 801,\n",
              " 'glitches': 802,\n",
              " 'belgi': 803,\n",
              " 'chiqib': 804,\n",
              " 'haqiqatdan': 805,\n",
              " 'borligini': 806,\n",
              " 'bolgani': 807,\n",
              " 'vaqtingizni': 808,\n",
              " 'qilishdan': 809,\n",
              " 'qilishda': 810,\n",
              " 'multiplayer': 811,\n",
              " 'yoqligini': 812,\n",
              " 'bera': 813,\n",
              " 'qismi': 814,\n",
              " 'bosib': 815,\n",
              " 'sabab': 816,\n",
              " 'sozlarni': 817,\n",
              " 'murakkab': 818,\n",
              " 'iloji': 819,\n",
              " 'yaqinda': 820,\n",
              " 'anglatadi': 821,\n",
              " 'orada': 822,\n",
              " 'tashlang': 823,\n",
              " 'ingliz': 824,\n",
              " 'oldingi': 825,\n",
              " 'boshlaydi': 826,\n",
              " 'qaysi': 827,\n",
              " 'kundan': 828,\n",
              " 'olishdan': 829,\n",
              " 'rad': 830,\n",
              " 'qilasiz': 831,\n",
              " 'yaxshiroqdir': 832,\n",
              " 'qaytaqayta': 833,\n",
              " 'joyda': 834,\n",
              " 'ochiq': 835,\n",
              " 'qiluvchi': 836,\n",
              " 'sony': 837,\n",
              " 'xperia': 838,\n",
              " 'kambagal': 839,\n",
              " 'singari': 840,\n",
              " 'videolarni': 841,\n",
              " 'saqlash': 842,\n",
              " 'darajadagi': 843,\n",
              " 'asosan': 844,\n",
              " 'tolashim': 845,\n",
              " 'ball': 846,\n",
              " 'yoqing': 847,\n",
              " 'hazil': 848,\n",
              " 'tark': 849,\n",
              " 'borish': 850,\n",
              " 'korinmaydi': 851,\n",
              " 'orqaga': 852,\n",
              " 'maglubiyatga': 853,\n",
              " 'dasturini': 854,\n",
              " 'yerda': 855,\n",
              " 'fun': 856,\n",
              " 'yaxshilash': 857,\n",
              " 'ulardan': 858,\n",
              " 'yangilangan': 859,\n",
              " 'my': 860,\n",
              " 'paytida': 861,\n",
              " 'yulduzlar': 862,\n",
              " 'awesome': 863,\n",
              " 'toxtab': 864,\n",
              " 'qurilma': 865,\n",
              " 'qiz': 866,\n",
              " 'malumotlar': 867,\n",
              " 'toxtatish': 868,\n",
              " 'bosganimda': 869,\n",
              " 'muammolarga': 870,\n",
              " 'androidda': 871,\n",
              " 'oh': 872,\n",
              " 'imkoniyati': 873,\n",
              " 'chiquvchi': 874,\n",
              " 'ornatilgan': 875,\n",
              " 'ilovaning': 876,\n",
              " 'ornatdim': 877,\n",
              " 'qilganda': 878,\n",
              " 'ishlatilgan': 879,\n",
              " 'kopgina': 880,\n",
              " '12': 881,\n",
              " 'ozi': 882,\n",
              " 'barchasini': 883,\n",
              " 'savollar': 884,\n",
              " 'karta': 885,\n",
              " 'berishga': 886,\n",
              " 'mumkinmi': 887,\n",
              " 'hikoya': 888,\n",
              " 'noqulay': 889,\n",
              " 'malumotlarini': 890,\n",
              " 'umidsizlik': 891,\n",
              " 'ozgartirish': 892,\n",
              " 'oynashdan': 893,\n",
              " 'foydalanib': 894,\n",
              " 'chaqaloq': 895,\n",
              " 'paytlarda': 896,\n",
              " 'bolganimda': 897,\n",
              " 'rejimi': 898,\n",
              " 'gazablangan': 899,\n",
              " 'game': 900,\n",
              " 'etdi': 901,\n",
              " 'bizga': 902,\n",
              " 'muhabbat': 903,\n",
              " 'jungle': 904,\n",
              " 'chiquvchilar': 905,\n",
              " 'yangilang': 906,\n",
              " 'malumotni': 907,\n",
              " 'pulning': 908,\n",
              " 'premium': 909,\n",
              " 'bularni': 910,\n",
              " 'ochirildi': 911,\n",
              " 'soatlab': 912,\n",
              " 'im': 913,\n",
              " 'mutlaqo': 914,\n",
              " 'soatdan': 915,\n",
              " 'yangiliklar': 916,\n",
              " 'tanga': 917,\n",
              " 'foydalanishni': 918,\n",
              " 'ozida': 919,\n",
              " 'mahalliy': 920,\n",
              " 'aytaman': 921,\n",
              " 'avtomobil': 922,\n",
              " 'ishdan': 923,\n",
              " 'otib': 924,\n",
              " 'korishim': 925,\n",
              " 'ishlashni': 926,\n",
              " 'buzilib': 927,\n",
              " 'nazar': 928,\n",
              " 'hisoblanadi': 929,\n",
              " 'real': 930,\n",
              " 'soniyadan': 931,\n",
              " 'umumiy': 932,\n",
              " 'moljallangan': 933,\n",
              " 'versiyada': 934,\n",
              " 'kompyuter': 935,\n",
              " 'silliq': 936,\n",
              " 'bolmaganda': 937,\n",
              " 'ota': 938,\n",
              " 'ettiraman': 939,\n",
              " 'ozim': 940,\n",
              " 'ilovasini': 941,\n",
              " 'videoni': 942,\n",
              " 'imkoniyat': 943,\n",
              " 'tarmoq': 944,\n",
              " 'xosh': 945,\n",
              " 'one': 946,\n",
              " 'vidjet': 947,\n",
              " 'yoqolgan': 948,\n",
              " 'miya': 949,\n",
              " 'aftidan': 950,\n",
              " 'otishni': 951,\n",
              " 'boshlaganimda': 952,\n",
              " 'mobaynida': 953,\n",
              " 'brilliant': 954,\n",
              " 'qancha': 955,\n",
              " 'batareyani': 956,\n",
              " 'olmadi': 957,\n",
              " 'wwwuz': 958,\n",
              " 'arzon': 959,\n",
              " 'kechikish': 960,\n",
              " 'yoqoladi': 961,\n",
              " 'gozal': 962,\n",
              " 'yoqotdim': 963,\n",
              " 'eta': 964,\n",
              " 'dostlar': 965,\n",
              " 'ekrani': 966,\n",
              " 'xohlaysizmi': 967,\n",
              " 'yangilashni': 968,\n",
              " 'olingan': 969,\n",
              " 'dasturda': 970,\n",
              " 'muammoga': 971,\n",
              " 'unda': 972,\n",
              " 'qilishga': 973,\n",
              " 'maglub': 974,\n",
              " 'korardim': 975,\n",
              " 'mb': 976,\n",
              " 'pullarni': 977,\n",
              " 'ikkala': 978,\n",
              " 'barmoq': 979,\n",
              " 'olishning': 980,\n",
              " 'chiqdi': 981,\n",
              " 'ornatib': 982,\n",
              " 'ozozidan': 983,\n",
              " 'ai': 984,\n",
              " '3d': 985,\n",
              " 'soraydi': 986,\n",
              " 'shahar': 987,\n",
              " 'tosatdan': 988,\n",
              " 'interfeys': 989,\n",
              " 'xatolarni': 990,\n",
              " 'otishi': 991,\n",
              " 'boling': 992,\n",
              " 'tashladi': 993,\n",
              " 'qimmat': 994,\n",
              " 'cheksiz': 995,\n",
              " 'ui': 996,\n",
              " 'oladigan': 997,\n",
              " 'ishladim': 998,\n",
              " 'oylab': 999,\n",
              " 'bularning': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdNHZ9A5USpw",
        "outputId": "faec8afa-51f1-40bc-cb4e-bbe13f95e611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 44, 128)           256000    \n",
            "                                                                 \n",
            " spatial_dropout1d_1 (Spati  (None, 44, 128)           0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 196)               254800    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 394       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 511194 (1.95 MB)\n",
            "Trainable params: 511194 (1.95 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Explanation**\n",
        "**1)** Imagine we opened a new box:\n",
        "   ```python\n",
        "model = Sequential()\n",
        "   ```  \n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dCEkQB6sEXVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**2)** We put the first layer into the box and put 3 items: 2000 frequent tokens, embed_dim, and input_length = X.shape[1] which is 44.\n",
        "   ```python\n",
        "model.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\n",
        "   ```  \n",
        "* Wait, where 44 came from and what is it?\n",
        "When we input `X.shape`, we output `(18485, 44)` where 18485 is the number of sequences (rows) and 44 is the length of the longest sequence after padding.\n",
        "\n",
        "* Note! The Embedding layer is used to convert integer-encoded words into dense vectors of fixed size. This is an essential step in Natural Language Processing (NLP) when working with text data.\n",
        "\n",
        "* Why exactly 128? Choosing an embedding dimension like 128 is a common practice and can often be a reasonable starting point, but itâ€™s not universally optimal for all projects.\n",
        "\n",
        "the `Embedding` layer transforms each word into a dense vector, and this process involves some initial randomness. Here's a more detailed breakdown of how it works:\n",
        "\n",
        "**1. Embedding Layer Initialization**\n",
        "\n",
        "When you first create an `Embedding` layer, the dense vectors for words are initialized randomly. For example, if you have `embed_dim = 4`, each word in the vocabulary will be mapped to a vector of size 4, but the values in these vectors are initially random.\n",
        "\n",
        " **2. How it Works**\n",
        "\n",
        "Hereâ€™s a step-by-step explanation:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - **Vocabulary Size**: Suppose you have a vocabulary of size 10,000 words.\n",
        "   - **Embedding Dimension**: You choose `embed_dim = 128`.\n",
        "   - **Embedding Matrix**: The `Embedding` layer creates a matrix of size `(10,000, 128)`. Each row of this matrix corresponds to a word in the vocabulary, and each row is initialized with random values.\n",
        "\n",
        "2. **Training**:\n",
        "   - **Forward Pass**: When you pass a sentence through the network, the `Embedding` layer looks up the dense vector for each token in the sentence. For example, the word \"love\" might be mapped to a vector like `[0.2, -0.5, 0.1, 0.3, ...]`.\n",
        "   - **Learning**: During training, the model adjusts the values in the embedding matrix based on the loss and gradients. This adjustment is done via backpropagation. The vectors for words are updated so that they better capture semantic and syntactic meanings based on the training data.\n",
        "\n",
        "3. **Example**\n",
        "\n",
        "Letâ€™s use a simple example:\n",
        "\n",
        "- Suppose \"love\" is tokenized to `1`.\n",
        "- The initial embedding vector for \"love\" might be something like `[0.12, -0.34, 0.56, 0.78, ...]`.\n",
        "\n",
        "**How the Embedding Matrix is Updated**\n",
        "\n",
        "During training:\n",
        "\n",
        "- If the context in which \"love\" appears suggests that \"love\" should be close to words like \"happy\" and \"joy\" in the vector space, the training process will adjust the vector for \"love\" to be closer to these words.\n",
        "- If \"love\" is less related to words like \"sad\" and \"angry\", its vector will move away from these words.\n",
        "\n",
        "The `Embedding` layer transforms words into dense vectors by initially assigning random values and then refining them through training. This process helps the model to learn useful representations of words based on their context and relationships within the training data.\n",
        "\n",
        "---\n",
        "Hereâ€™s a more detailed explanation of how embedding vectors are adjusted and why theyâ€™re crucial:\n",
        "\n",
        "### **1. Understanding the Embedding Vectors**\n",
        "\n",
        "**Initial Random Values**:\n",
        "- When you start, the embedding vectors for words like \"love,\" \"happy,\" and \"joy\" are initialized with random values. For example, \"love\" might start with a vector like `[0.12, -0.34, 0.56, ...]`.\n",
        "\n",
        "**Vector Size**:\n",
        "- The `embed_dim` defines the size of each vector. In your example, this is `128`, so each word is represented by a 128-dimensional vector.\n",
        "\n",
        "### **2. Adjusting Vectors During Training**\n",
        "\n",
        "**How Training Works**:\n",
        "- **Forward Pass**: During training, sentences (e.g., \"I love this laptop\") are converted into sequences of these embedding vectors. For instance, \"love\" might be represented by `[0.12, -0.34, 0.56, ...]`.\n",
        "  \n",
        "- **Loss Calculation**: The model makes predictions based on these vectors and calculates a loss based on how well the predictions match the actual labels (e.g., sentiment labels).\n",
        "\n",
        "- **Backpropagation**: The loss is used to update the model parameters, including the embedding vectors. This process involves calculating gradients of the loss with respect to the embeddings.\n",
        "\n",
        "**Updating Embeddings**:\n",
        "- **Gradient Descent**: Gradients are used to adjust the embedding vectors to minimize the loss. If the model finds that \"love\" should be closer to \"happy\" and \"joy,\" it will adjust the vector for \"love\" to move it closer to the vectors for \"happy\" and \"joy\" in the vector space.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "1. **Initial Embeddings**:\n",
        "   - \"love\" = `[0.12, -0.34, 0.56, ...]`\n",
        "   - \"happy\" = `[0.23, -0.44, 0.54, ...]`\n",
        "   - \"joy\" = `[0.21, -0.40, 0.50, ...]`\n",
        "\n",
        "2. **During Training**:\n",
        "   - The model might find that sentences with \"love\" are often used in positive contexts, similar to \"happy\" and \"joy\".\n",
        "   - The loss function and gradient calculations will adjust the embeddings so that \"love\" ends up with a vector closer to those of \"happy\" and \"joy\".\n",
        "\n",
        "3. **Adjusted Embeddings**:\n",
        "   - After several training iterations, the embeddings might be updated to:\n",
        "     - \"love\" = `[0.25, -0.30, 0.55, ...]`\n",
        "     - \"happy\" = `[0.26, -0.32, 0.57, ...]`\n",
        "     - \"joy\" = `[0.27, -0.31, 0.58, ...]`\n",
        "\n",
        "### **3. Why This Adjustment Matters**\n",
        "\n",
        "- **Semantic Similarity**: By adjusting vectors based on context, the model learns that \"love,\" \"happy,\" and \"joy\" are semantically similar and places them closer together in the vector space. This helps the model understand that these words often occur in similar contexts.\n",
        "\n",
        "- **Capturing Relationships**: This process allows the embeddings to capture complex relationships between words. For example, words with similar sentiments or meanings will have similar vectors.\n",
        "\n",
        "- **Feature Representation**: Embedding vectors turn categorical word data into continuous feature representations that can be used effectively in neural networks, enabling the model to learn and generalize better.\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "The embedding vectors start with random values but are refined through training. The model uses context and gradients to adjust these vectors so that semantically similar words have similar embeddings. This adjustment is essential for the model to understand and work with text data effectively.\n",
        "\n",
        "Let's delve into how the model identifies relationships between words like \"love,\" \"joy,\" and \"happy,\" and how similar sentiments are reflected in word embeddings.\n",
        "\n",
        "### **1. Identifying Relationships Between Words**\n",
        "\n",
        "**Contextual Learning**:\n",
        "- **Training Data**: During training, the model is exposed to many sentences and their labels. It learns from these examples how words relate to each other in the context of predicting sentiments or other tasks.\n",
        "- **Contextual Co-occurrence**: Words that often appear in similar contexts tend to be adjusted to have similar embeddings. For instance, if \"love\" and \"happy\" frequently appear in positive sentences, the model will learn to adjust their vectors so that they are close to each other.\n",
        "\n",
        "**Example Workflow**:\n",
        "\n",
        "1. **Training Sentences**:\n",
        "   - Sentence 1: \"I love you\" (positive)\n",
        "   - Sentence 2: \"I liked this app\" (positive)\n",
        "   - Sentence 3: \"I am happy for this\" (positive)\n",
        "\n",
        "2. **Learning Process**:\n",
        "   - During training, the model uses these sentences to learn that positive sentiments are associated with words like \"love,\" \"liked,\" and \"happy.\"\n",
        "   - **Similarity in Context**: Since \"love,\" \"liked,\" and \"happy\" are frequently seen in positive contexts, the model adjusts their vectors to be similar. The embedding for \"love\" will be adjusted to be closer to \"happy\" because they appear in similar contexts.\n",
        "\n",
        "3. **Word Embeddings Adjustment**:\n",
        "   - If the training data shows that positive sentences often contain these words, their embeddings will be updated so that:\n",
        "     - \"love\" = `[0.25, -0.30, 0.55, ...]`\n",
        "     - \"happy\" = `[0.27, -0.31, 0.58, ...]`\n",
        "     - \"liked\" = `[0.26, -0.29, 0.56, ...]`\n",
        "   - Words with similar contexts (like positive sentiments) will end up having similar vectors.\n",
        "\n",
        "### **2. Similarity in Word Vectors**\n",
        "\n",
        "**Similar Sentiments**:\n",
        "- **Similar Vectors**: Words with similar sentiments or meanings end up with similar vectors because the embedding layer adjusts these vectors based on their co-occurrence in similar contexts.\n",
        "\n",
        "**Why This Happens**:\n",
        "- **Training Objective**: The model's objective during training is to minimize the error in predicting the sentiment (or other tasks). This leads to similar vectors for words that are used in similar contexts.\n",
        "- **Vector Space**: In the vector space created by the embedding layer, words with similar meanings or sentiments are placed closer together. This is because the training process learns to represent them with similar vectors to improve prediction accuracy.\n",
        "\n",
        "**Illustrative Example**:\n",
        "\n",
        "1. **Initial Embeddings** (Random):\n",
        "   - \"love\" = `[0.12, -0.34, 0.56, ...]`\n",
        "   - \"happy\" = `[0.22, -0.33, 0.55, ...]`\n",
        "   - \"angry\" = `[-0.45, 0.67, -0.23, ...]`\n",
        "\n",
        "2. **After Training** (Adjusted):\n",
        "   - \"love\" = `[0.25, -0.30, 0.55, ...]`\n",
        "   - \"happy\" = `[0.27, -0.31, 0.58, ...]`\n",
        "   - \"angry\" = `[-0.40, 0.60, -0.25, ...]`\n",
        "\n",
        "   Here, \"love\" and \"happy\" are closer to each other, while \"angry\" is farther away because it represents a different sentiment.\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "1. **Identifying Relationships**: The model identifies relationships between words through training, where it learns to adjust word vectors based on the context in which words appear. Words with similar contexts and sentiments end up with similar vectors.\n",
        "\n",
        "2. **Similar Vectors for Similar Sentiments**: Positive sentiment words like \"love\" and \"happy\" end up with similar vectors because the training process adjusts their embeddings to reflect their similar use in positive contexts.\n",
        "\n",
        "-------\n",
        "------\n",
        "------"
      ],
      "metadata": {
        "id": "cmICHtIm8LcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**3)** We put the second layer into the box and put 1 items:\n",
        "   ```python\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "   ```\n",
        "Let's delve into the `SpatialDropout1D` layer and understand its role in the model. Hereâ€™s a breakdown:\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of `model.add(SpatialDropout1D(0.4))`**\n",
        "\n",
        "**1. Purpose of SpatialDropout1D:**\n",
        "\n",
        "- **What it Does:** `SpatialDropout1D` is used to prevent overfitting in sequential data models by randomly setting a fraction of the input time steps to zero during training.\n",
        "- **How it Helps:** By dropping entire rows of the input data, it forces the model to learn more robust and generalized features, as it cannot rely on any specific time step being present.\n",
        "\n",
        "**2. Dropout Rate:**\n",
        "\n",
        "- **Dropout Rate (0.4):** In this example, `0.4` means that during each training step, 40% of the rows (time steps) in the input sequence will be randomly set to zero.\n",
        "- **Impact:** This prevents the model from becoming too dependent on any particular time step, improving its ability to generalize.\n",
        "\n",
        "**3. How it Works:**\n",
        "\n",
        "- **Training Step:** During each training step (or batch), `SpatialDropout1D` will randomly select 40% of the time steps to drop.\n",
        "- **Randomness:** The specific time steps dropped are chosen randomly, which changes from one training step to another.\n",
        "\n",
        "**4. Illustration:**\n",
        "\n",
        "- **Original Input Sequence (5 time steps, 4 features each):**\n",
        "  ```\n",
        "  [\n",
        "   [0.1, 0.2, 0.3, 0.4],  # Time step 1\n",
        "   [0.2, 0.1, 0.4, 0.3],  # Time step 2\n",
        "   [0.3, 0.4, 0.1, 0.2],  # Time step 3\n",
        "   [0.4, 0.3, 0.2, 0.1],  # Time step 4\n",
        "   [0.5, 0.6, 0.7, 0.8]   # Time step 5\n",
        "  ]\n",
        "  ```\n",
        "\n",
        "- **After Applying `SpatialDropout1D(0.4)` (Training Step 1):** Randomly drops 40% of the time steps (e.g., Time step 2 and Time step 4):\n",
        "  ```\n",
        "  [\n",
        "   [0.1, 0.2, 0.3, 0.4],  # Time step 1\n",
        "   [0.0, 0.0, 0.0, 0.0],  # Time step 2 (dropped)\n",
        "   [0.3, 0.4, 0.1, 0.2],  # Time step 3\n",
        "   [0.0, 0.0, 0.0, 0.0],  # Time step 4 (dropped)\n",
        "   [0.5, 0.6, 0.7, 0.8]   # Time step 5\n",
        "  ]\n",
        "  ```\n",
        "\n",
        "- **After Applying `SpatialDropout1D(0.4)` (Training Step 2):** Randomly drops a different set of time steps (e.g., Time step 1 and Time step 5):\n",
        "  ```\n",
        "  [\n",
        "   [0.0, 0.0, 0.0, 0.0],  # Time step 1 (dropped)\n",
        "   [0.2, 0.1, 0.4, 0.3],  # Time step 2\n",
        "   [0.3, 0.4, 0.1, 0.2],  # Time step 3\n",
        "   [0.4, 0.3, 0.2, 0.1],  # Time step 4\n",
        "   [0.0, 0.0, 0.0, 0.0]   # Time step 5 (dropped)\n",
        "  ]\n",
        "  ```\n",
        "\n",
        "**5. Summary:**\n",
        "\n",
        "- **SpatialDropout1D**: Randomly drops 40% of the input time steps in each training step to prevent overfitting.\n",
        "- **Dropout Rate (0.4)**: Refers to the fraction of time steps that are set to zero.\n",
        "- **Training Steps**: The specific time steps dropped vary between training steps, introducing randomness to enhance model generalization.\n",
        "---\n",
        "Let's clarify your questions about `SpatialDropout1D(0.4)` and training steps:\n",
        "\n",
        "### 1. Number of Time Steps Dropped\n",
        "\n",
        "If you have 18,000 time steps in your input sequences and you apply `SpatialDropout1D(0.4)`, here's how it works:\n",
        "\n",
        "- **Dropout Rate (0.4):** This means 40% of the time steps are set to zero during each training step.\n",
        "\n",
        "**Number of Time Steps Dropped:**\n",
        "- **Calculation:** `40% of 18,000 = 0.4 * 18,000 = 7,200`\n",
        "- **Explanation:** During each training step, `SpatialDropout1D(0.4)` will randomly set 7,200 time steps to zero out of the 18,000.\n",
        "\n",
        "### 2. Number of Training Steps (or Epochs)\n",
        "\n",
        "**Training Steps and Epochs:**\n",
        "\n",
        "- **Training Steps:** Refers to the number of batches processed in one epoch.\n",
        "- **Epochs:** Refers to the number of times the entire dataset is passed through the model.\n",
        "\n",
        "**How to Determine Number of Training Steps:**\n",
        "\n",
        "1. **Define the Epochs:**\n",
        "   - The number of epochs is a hyperparameter you set before training. For example, you might choose to train for 10 epochs.\n",
        "\n",
        "2. **Batch Size:**\n",
        "   - The number of training steps in an epoch depends on the batch size and the number of samples.\n",
        "   - **Formula:** Number of training steps per epoch = (Total number of samples) / (Batch size)\n",
        "   - For instance, if you have 18,000 samples and your batch size is 32, then the number of training steps per epoch would be `18,000 / 32 â‰ˆ 562.5`. Since you can't have half a step, this will be rounded to 563 steps.\n",
        "\n",
        "3. **Total Training Steps:**\n",
        "   - To find the total number of training steps during the entire training process, multiply the number of training steps per epoch by the number of epochs.\n",
        "   - **Example:** If you train for 10 epochs with 563 training steps per epoch, the total number of training steps would be `10 * 563 = 5,630`.\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "- **During each training step,** `SpatialDropout1D(0.4)` will randomly drop 40% of the time steps. For 18,000 time steps, this means 7,200 will be dropped in each step.\n",
        "- **Number of training steps per epoch** depends on your batch size and the total number of samples.\n",
        "- **Total number of training steps** is calculated by multiplying the number of training steps per epoch by the number of epochs.\n",
        "\n",
        "---\n",
        "To determine how many TRAINING STEPS(which in every training step, 40% of entire embedings will be dropped) occur during training and how epochs relate to these steps:\n",
        "\n",
        "### Training Steps per Epoch\n",
        "\n",
        "1. **Batch Size:**\n",
        "   - The batch size is the number of samples processed before the model's weights are updated. You set this in the `model.fit()` function.\n",
        "   - Example: `batch_size=64`\n",
        "\n",
        "2. **Total Training Steps per Epoch:**\n",
        "   - **Formula:** Number of training steps per epoch = Total number of samples / Batch size\n",
        "   - **Example Calculation:**\n",
        "     - If you have 12,000 samples and `batch_size=64`, then:\n",
        "     - Number of steps per epoch = 12,000 / 64 â‰ˆ 188 steps per epoch\n",
        "\n",
        "### Epochs\n",
        "\n",
        "- **Epoch:** One complete pass through the entire dataset.\n",
        "- **Total Steps in All Epochs:**\n",
        "  - **Formula:** Total number of training steps = Number of training steps per epoch Ã— Number of epochs\n",
        "  - **Example Calculation:**\n",
        "    - If you train for 5 epochs, then:\n",
        "    - Total training steps = 188 steps/epoch Ã— 5 epochs = 940 steps\n",
        "\n",
        "### Summary\n",
        "\n",
        "1. **Determine Training Steps per Epoch:** Divide the total number of samples by the batch size.\n",
        "2. **Total Training Steps:** Multiply the number of training steps per epoch by the number of epochs.\n",
        "\n",
        "----\n",
        "----\n",
        "----\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i_rqyVzMEd9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4)** We put the third layer into the box and put 1 items:\n",
        "   ```python\n",
        "lstm_out = 196\n",
        "\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "   ```\n",
        "\n"
      ],
      "metadata": {
        "id": "DIRTy8HZVZYr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}